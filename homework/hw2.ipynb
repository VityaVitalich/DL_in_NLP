{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GIr56czmR5FZ",
        "r7p9Nac-4X8C",
        "r6enjcHrD_0Y",
        "-Qsehb_eERRF",
        "UiYl5XsdkmZG",
        "g026bdkrEtiQ",
        "TEpsKqLwE3hB",
        "Kh86csH_FCyB",
        "IwoQ_X8ylJYN",
        "LUdLLSojGJbM",
        "6kyeSkMeGQo_",
        "bwP_NVeYGY-j",
        "byCY6Tn-A9i_",
        "jpG6i8X-HMmF",
        "oFfDH0-SsRm-",
        "7hMYmIO2tf8z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KslYmMKMLY7V"
      },
      "source": [
        "#HW2 - Transliteration\n",
        "\n",
        "Please send this to dlnlp2023@mail.ru with subject \"Surname_HW2\"\n",
        "\n",
        "Deadline: 16.01.2023\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeC1UiR2LXiV"
      },
      "source": [
        "In this task you are required to solve the transliteration problem of names from English to Russian. Transliteration of a string means writing this string using the alphabet of another language with the preservation of pronunciation, although not always.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIr56czmR5FZ"
      },
      "source": [
        "## Basic algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/s-nlp/filimdb_evaluation/raw/master/TRANSLIT.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lf3-DST1YBx",
        "outputId": "6734734f-2fc7-4ff8-efe1-7eea81687195"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-24 09:54:12--  https://github.com/s-nlp/filimdb_evaluation/raw/master/TRANSLIT.tar.gz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/s-nlp/filimdb_evaluation/master/TRANSLIT.tar.gz [following]\n",
            "--2023-12-24 09:54:13--  https://raw.githubusercontent.com/s-nlp/filimdb_evaluation/master/TRANSLIT.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1546458 (1.5M) [application/octet-stream]\n",
            "Saving to: ‘TRANSLIT.tar.gz’\n",
            "\n",
            "TRANSLIT.tar.gz     100%[===================>]   1.47M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2023-12-24 09:54:13 (177 MB/s) - ‘TRANSLIT.tar.gz’ saved [1546458/1546458]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gunzip TRANSLIT.tar.gz"
      ],
      "metadata": {
        "id": "a1wUwZbT1lDd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf TRANSLIT.tar"
      ],
      "metadata": {
        "id": "pg5z4ezh1zO6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation code"
      ],
      "metadata": {
        "id": "r7p9Nac-4X8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREDS_FNAME = \"preds_translit_baseline.tsv\"\n",
        "SCORED_PARTS = ('train', 'dev', 'train_small', 'dev_small', 'test')\n",
        "TRANSLIT_PATH = \"TRANSLIT\""
      ],
      "metadata": {
        "id": "aOz9Miec58Tb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "from pandas import read_csv\n",
        "\n",
        "def load_dataset(data_dir_path=None, parts: List[str] = SCORED_PARTS):\n",
        "    part2ixy = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        with open(path, 'r', encoding='utf-8') as rf:\n",
        "            # first line is a header of the corresponding columns\n",
        "            lines = rf.readlines()[1:]\n",
        "            col_count = len(lines[0].strip('\\n').split('\\t'))\n",
        "            if col_count == 2:\n",
        "                strings, transliterations = zip(\n",
        "                    *list(map(lambda l: l.strip('\\n').split('\\t'), lines))\n",
        "                )\n",
        "            elif col_count == 1:\n",
        "                strings = list(map(lambda l: l.strip('\\n'), lines))\n",
        "                transliterations = None\n",
        "            else:\n",
        "                raise ValueError(\"wrong amount of columns\")\n",
        "        part2ixy[part] = (\n",
        "            [f'{part}/{i}' for i in range(len(strings))],\n",
        "            strings, transliterations,\n",
        "        )\n",
        "    return part2ixy\n",
        "\n",
        "\n",
        "def load_transliterations_only(data_dir_path=None, parts: List[str] = SCORED_PARTS):\n",
        "    part2iy = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        with open(path, 'r', encoding='utf-8') as rf:\n",
        "            # first line is a header of the corresponding columns\n",
        "            lines = rf.readlines()[1:]\n",
        "            col_count = len(lines[0].strip('\\n').split('\\t'))\n",
        "            n_lines = len(lines)\n",
        "            if col_count == 2:\n",
        "                transliterations = [l.strip('\\n').split('\\t')[1] for l in lines]\n",
        "            elif col_count == 1:\n",
        "                transliterations = None\n",
        "            else:\n",
        "                raise ValueError(\"Wrong amount of columns\")\n",
        "        part2iy[part] = (\n",
        "            [f'{part}/{i}' for i in range(n_lines)],\n",
        "            transliterations,\n",
        "        )\n",
        "    return part2iy\n",
        "\n",
        "\n",
        "def save_preds(preds, preds_fname):\n",
        "    \"\"\"\n",
        "    Save classifier predictions in format appropriate for scoring.\n",
        "    \"\"\"\n",
        "    with codecs.open(preds_fname, 'w') as outp:\n",
        "        for idx, preds in preds:\n",
        "            print(idx, *preds, sep='\\t', file=outp)\n",
        "    print('Predictions saved to %s' % preds_fname)\n",
        "\n",
        "\n",
        "def load_preds(preds_fname, top_k=1):\n",
        "    \"\"\"\n",
        "    Load classifier predictions in format appropriate for scoring.\n",
        "    \"\"\"\n",
        "    kwargs = {\n",
        "        \"filepath_or_buffer\": preds_fname,\n",
        "        \"names\": [\"id\", \"pred\"],\n",
        "        \"sep\": '\\t',\n",
        "    }\n",
        "\n",
        "    pred_ids = list(read_csv(**kwargs, usecols=[\"id\"])[\"id\"])\n",
        "\n",
        "    pred_y = {\n",
        "        pred_id: [y]\n",
        "        for pred_id, y in zip(\n",
        "            pred_ids, read_csv(**kwargs, usecols=[\"pred\"])[\"pred\"]\n",
        "        )\n",
        "    }\n",
        "\n",
        "    for y in pred_y.values():\n",
        "        assert len(y) == top_k\n",
        "\n",
        "    return pred_ids, pred_y\n",
        "\n",
        "\n",
        "def compute_hit_k(preds, k=10):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def compute_mrr(preds):\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def compute_acc_1(preds, true):\n",
        "    right_answers = 0\n",
        "    bonus = 0\n",
        "    for pred, y in zip(preds, true):\n",
        "        if pred[0] == y:\n",
        "            right_answers += 1\n",
        "        elif pred[0] != pred[0] and y == 'нань':\n",
        "            print('Your test file contained empty string, skipping %f and %s' % (pred[0], y))\n",
        "            bonus += 1 # bugfix: skip empty line in test\n",
        "    return right_answers / (len(preds) - bonus)\n",
        "\n",
        "\n",
        "def score(preds, true):\n",
        "    assert len(preds) == len(true), 'inconsistent amount of predictions and ground truth answers'\n",
        "    acc_1 = compute_acc_1(preds, true)\n",
        "    return {'acc@1': acc_1}\n",
        "\n",
        "\n",
        "def score_preds(preds_path, data_dir, parts=SCORED_PARTS):\n",
        "    part2iy = load_transliterations_only(data_dir, parts=parts)\n",
        "    pred_ids, pred_dict = load_preds(preds_path)\n",
        "    # pred_dict = {i:y for i,y in zip(pred_ids, pred_y)}\n",
        "    scores = {}\n",
        "    for part, (true_ids, true_y) in part2iy.items():\n",
        "        if true_y is None:\n",
        "            print('no labels for %s set' % part)\n",
        "            continue\n",
        "        pred_y = [pred_dict[i] for i in true_ids]\n",
        "        score_values = score(pred_y, true_y)\n",
        "        acc_1 = score_values['acc@1']\n",
        "        print('%s set accuracy@1: %.2f' % (part, acc_1))\n",
        "        scores[part] = score_values\n",
        "    return scores"
      ],
      "metadata": {
        "id": "SdbtMBxd52yX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwXaPC4LiUMe"
      },
      "source": [
        "## Transformer-based approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM-9cKhbidfl"
      },
      "source": [
        "\n",
        "To implement your algorithm, use the template code, which needs to be modified.\n",
        "\n",
        "First, you need to add some details in the code of the Transformer architecture, implement the methods of the class `LrScheduler`, which is responsible for updating the learning rate during training.\n",
        "Next, you need to select the hyperparameters for the model according to the proposed guide."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nePd6qR5_sC-",
        "outputId": "4948e4b7-cbc7-4c19-a8c4-56a737ad2ea8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.23.0 rapidfuzz-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools as it\n",
        "import collections as col\n",
        "import random\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import datetime, time\n",
        "\n",
        "import copy\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as torch_data\n",
        "import itertools as it\n",
        "import collections as col\n",
        "import random\n",
        "\n",
        "import Levenshtein as le"
      ],
      "metadata": {
        "id": "LQ5sfyjhBawp"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset and embeddings"
      ],
      "metadata": {
        "id": "r6enjcHrD_0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_datasets(data_dir_path, parts):\n",
        "    datasets = {}\n",
        "    for part in parts:\n",
        "        path = os.path.join(data_dir_path, f'{part}.tsv')\n",
        "        datasets[part] = pd.read_csv(path, sep='\\t', na_filter=False)\n",
        "        print(f'Loaded {part} dataset, length: {len(datasets[part])}')\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "RmWG0dDUCFto"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder:\n",
        "    def __init__(self, load_dir_path=None):\n",
        "        self.lang_keys = ['en', 'ru']\n",
        "        self.directions = ['id2token', 'token2id']\n",
        "        self.service_token_names = {\n",
        "            'pad_token': '<pad>',\n",
        "            'start_token': '<start>',\n",
        "            'unk_token': '<unk>',\n",
        "            'end_token': '<end>'\n",
        "        }\n",
        "        service_id2token = dict(enumerate(self.service_token_names.values()))\n",
        "        service_token2id ={v:k for k,v in service_id2token.items()}\n",
        "        self.service_vocabs = dict(zip(self.directions,\n",
        "                                       [service_id2token, service_token2id]))\n",
        "        if load_dir_path is None:\n",
        "            self.vocabs = {}\n",
        "            for lk in self.lang_keys:\n",
        "                self.vocabs[lk] = copy.deepcopy(self.service_vocabs)\n",
        "        else:\n",
        "            self.vocabs = self.load_vocabs(load_dir_path)\n",
        "    def load_vocabs(self, load_dir_path):\n",
        "        vocabs = {}\n",
        "        load_path = os.path.join(load_dir_path, 'vocabs')\n",
        "        for lk in self.lang_keys:\n",
        "            vocabs[lk] = {}\n",
        "            for d in self.directions:\n",
        "                columns = d.split('2')\n",
        "                print(lk, d)\n",
        "                df = pd.read_csv(os.path.join(load_path, f'{lk}_{d}'))\n",
        "                vocabs[lk][d] = dict(zip(*[df[c] for c in columns]))\n",
        "        return vocabs\n",
        "\n",
        "    def save_vocabs(self, save_dir_path):\n",
        "        save_path = os.path.join(save_dir_path, 'vocabs')\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        for lk in self.lang_keys:\n",
        "            for d in self.directions:\n",
        "                columns = d.split('2')\n",
        "                pd.DataFrame(data=self.vocabs[lk][d].items(),\n",
        "                    columns=columns).to_csv(os.path.join(save_path, f'{lk}_{d}'),\n",
        "                                                index=False,\n",
        "                                                sep=',')\n",
        "    def make_vocabs(self, data_df):\n",
        "        for lk in self.lang_keys:\n",
        "            tokens = col.Counter(''.join(list(it.chain(*data_df[lk])))).keys()\n",
        "            part_id2t = dict(enumerate(tokens, start=len(self.service_token_names)))\n",
        "            part_t2id = {k:v for v,k in part_id2t.items()}\n",
        "            part_vocabs = [part_id2t, part_t2id]\n",
        "            for i in range(len(self.directions)):\n",
        "                self.vocabs[lk][self.directions[i]].update(part_vocabs[i])\n",
        "\n",
        "        self.src_vocab_size = len(self.vocabs['en']['id2token'])\n",
        "        self.tgt_vocab_size = len(self.vocabs['ru']['id2token'])\n",
        "\n",
        "    def frame(self, sample, start_token=None, end_token=None):\n",
        "        if start_token is None:\n",
        "            start_token=self.service_token_names['start_token']\n",
        "        if end_token is None:\n",
        "            end_token=self.service_token_names['end_token']\n",
        "        return [start_token] + sample + [end_token]\n",
        "    def token2id(self, samples, frame, lang_key):\n",
        "        if frame:\n",
        "            samples = list(map(self.frame, samples))\n",
        "        vocab = self.vocabs[lang_key]['token2id']\n",
        "        return list(map(lambda s:\n",
        "                        [vocab[t] if t in vocab.keys() else vocab[self.service_token_names['unk_token']]\n",
        "                         for t in s], samples))\n",
        "\n",
        "    def unframe(self, sample, start_token=None, end_token=None):\n",
        "        if start_token is None:\n",
        "            start_token=self.service_vocabs['token2id'][self.service_token_names['start_token']]\n",
        "        if end_token is None:\n",
        "            end_token=self.service_vocabs['token2id'][self.service_token_names['end_token']]\n",
        "        pad_token=self.service_vocabs['token2id'][self.service_token_names['pad_token']]\n",
        "        return list(it.takewhile(lambda e: e != end_token and e != pad_token, sample[1:]))\n",
        "    def id2token(self, samples, unframe, lang_key):\n",
        "        if unframe:\n",
        "            samples = list(map(self.unframe, samples))\n",
        "        vocab = self.vocabs[lang_key]['id2token']\n",
        "        return list(map(lambda s:\n",
        "                        [vocab[idx] if idx in vocab.keys() else self.service_token_names['unk_token'] for idx in s], samples))\n",
        "\n",
        "\n",
        "class TranslitData(torch_data.Dataset):\n",
        "    def __init__(self, source_strings, target_strings,\n",
        "                text_encoder):\n",
        "        super(TranslitData, self).__init__()\n",
        "        self.source_strings = source_strings\n",
        "        self.text_encoder = text_encoder\n",
        "        if target_strings is not None:\n",
        "            assert len(source_strings) == len(target_strings)\n",
        "            self.target_strings = target_strings\n",
        "        else:\n",
        "            self.target_strings = None\n",
        "    def __len__(self):\n",
        "        return len(self.source_strings)\n",
        "    def __getitem__(self, idx):\n",
        "        src_str = self.source_strings[idx]\n",
        "        encoder_input = self.text_encoder.token2id([list(src_str)], frame=True, lang_key='en')[0]\n",
        "        if self.target_strings is not None:\n",
        "            tgt_str = self.target_strings[idx]\n",
        "            tmp = self.text_encoder.token2id([list(tgt_str)], frame=True, lang_key='ru')[0]\n",
        "            decoder_input = tmp[:-1]\n",
        "            decoder_target = tmp[1:]\n",
        "            return (encoder_input, decoder_input, decoder_target)\n",
        "        else:\n",
        "            return (encoder_input,)\n",
        "\n",
        "\n",
        "class BatchSampler(torch_data.BatchSampler):\n",
        "    def __init__(self, sampler, batch_size, drop_last, shuffle_each_epoch):\n",
        "        super(BatchSampler, self).__init__(sampler, batch_size, drop_last)\n",
        "        self.batches = []\n",
        "        for b in super(BatchSampler, self).__iter__():\n",
        "            self.batches.append(b)\n",
        "        self.shuffle_each_epoch = shuffle_each_epoch\n",
        "        if self.shuffle_each_epoch:\n",
        "            random.shuffle(self.batches)\n",
        "        self.index = 0\n",
        "        #print(f'Batches collected: {len(self.batches)}')\n",
        "    def __iter__(self):\n",
        "        self.index = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self.index == len(self.batches):\n",
        "            if self.shuffle_each_epoch:\n",
        "                random.shuffle(self.batches)\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            batch = self.batches[self.index]\n",
        "            self.index += 1\n",
        "            return batch\n",
        "\n",
        "def collate_fn(batch_list):\n",
        "    '''batch_list can store either 3 components:\n",
        "        encoder_inputs, decoder_inputs, decoder_targets\n",
        "        or single component: encoder_inputs'''\n",
        "    components = list(zip(*batch_list))\n",
        "    batch_tensors = []\n",
        "    for data in components:\n",
        "        max_len = max([len(sample) for sample in data])\n",
        "        #print(f'Maximum length in batch = {max_len}')\n",
        "        sample_tensors = [torch.tensor(s, requires_grad=False, dtype=torch.int64)\n",
        "                         for s in data]\n",
        "        batch_tensors.append(nn.utils.rnn.pad_sequence(\n",
        "            sample_tensors,\n",
        "            batch_first=True, padding_value=0))\n",
        "    return tuple(batch_tensors)\n",
        "\n",
        "\n",
        "def create_dataloader(source_strings, target_strings,\n",
        "                      text_encoder, batch_size,\n",
        "                      shuffle_batches_each_epoch):\n",
        "    '''target_strings parameter can be None'''\n",
        "    dataset = TranslitData(source_strings, target_strings,\n",
        "                                text_encoder=text_encoder)\n",
        "    seq_sampler = torch_data.SequentialSampler(dataset)\n",
        "    batch_sampler = BatchSampler(seq_sampler, batch_size=batch_size,\n",
        "                                drop_last=False,\n",
        "                                shuffle_each_epoch=shuffle_batches_each_epoch)\n",
        "    dataloader = torch_data.DataLoader(dataset,\n",
        "                                       batch_sampler=batch_sampler,\n",
        "                                       collate_fn=collate_fn)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "-gMDFNVt-nMw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric function"
      ],
      "metadata": {
        "id": "-Qsehb_eERRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(predicted_strings, target_strings, metrics):\n",
        "    metric_values = {}\n",
        "    for m in metrics:\n",
        "        if m == 'acc@1':\n",
        "            metric_values[m] = sum(predicted_strings == target_strings) / len(target_strings)\n",
        "        elif m =='mean_ld@1':\n",
        "            metric_values[m] =\\\n",
        "                np.mean(list(map(lambda e: le.distance(*e), zip(predicted_strings, target_strings))))\n",
        "        else:\n",
        "            raise ValueError(f'Unknown metric: {m}')\n",
        "    return metric_values"
      ],
      "metadata": {
        "id": "MN4I60GqETB0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiYl5XsdkmZG"
      },
      "source": [
        "###  Positional Encoding [1 Point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkNaSzwrkpf_"
      },
      "source": [
        "As you remember, Transformer treats an input sequence of elements as a time series. Since the Encoder inside the Transformer simultaneously processes the entire input sequence, the information about the position of the element needs to be encoded inside its embedding, since it is not identified in any other way inside the model. That is why the PositionalEncoding layer is used, which sums embeddings with a vector of the same dimension.\n",
        "Let the matrix of these vectors for each position of the time series be denoted as $PE$. Then the elements of the matrix are:\n",
        "\n",
        "$$ PE_{(pos,2i)} = \\sin{(pos/10000^{2i/d_{model}})}$$\n",
        "$$ PE_{(pos,2i+1)} = \\cos{(pos/10000^{2i/d_{model}})}$$\n",
        "\n",
        "where $pos$ - is the position, $i$ - index of the component of the corresponging vector, $d_{model}$ - dimension of each vector. Thus, even components represent sine values, and odd ones represent cosine values with different arguments.\n",
        "\n",
        "To run the test use the following function:\n",
        "\n",
        "`test_positional_encoding()`\n",
        "\n",
        "Make sure that there is no any `AssertionError`!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_size):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.emb_layer = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.emb_layer(x)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_size, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, hidden_size, requires_grad=False)\n",
        "        # TODO: implement your code here\n",
        "        pe = pe.unsqueeze(0)\n",
        "        # pe shape: (1, max_len, hidden_size)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: shape (batch size, sequence length, hidden size)\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "Rm4g1vybAKZs"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_positional_encoding():\n",
        "    pe = PositionalEncoding(max_len=3, hidden_size=4)\n",
        "    res_1 = torch.tensor([[[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
        "                           [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
        "                           [ 0.9093, -0.4161,  0.0200,  0.9998]]])\n",
        "    # print(pe.pe - res_1)\n",
        "    assert torch.all(torch.abs(pe.pe - res_1) < 1e-4).item()\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "hBk6uxQyCqt-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_positional_encoding()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwN9Qk_NCw-K",
        "outputId": "007aab6f-8b35-4765-8469-d294f823ba70"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test is passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LayerNorm"
      ],
      "metadata": {
        "id": "g026bdkrEtiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Layer Normalization layer\"\n",
        "\n",
        "    def __init__(self, hidden_size, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gain = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        print(mean, std)\n",
        "        return self.gain * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "dIMy52O9Es0K"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SublayerConnection"
      ],
      "metadata": {
        "id": "TEpsKqLwE3hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer normalization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.layer_norm = LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return self.layer_norm(x + self.dropout(sublayer(x)))\n",
        "\n",
        "def padding_mask(x, pad_idx=0):\n",
        "    assert len(x.size()) >= 2\n",
        "    return (x != pad_idx).unsqueeze(-2)\n",
        "\n",
        "def look_ahead_mask(size):\n",
        "    \"Mask out the right context\"\n",
        "    attn_shape = (1, size, size)\n",
        "    look_ahead_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(look_ahead_mask) == 0\n",
        "\n",
        "def compositional_mask(x, pad_idx=0):\n",
        "    pm = padding_mask(x, pad_idx=pad_idx)\n",
        "    seq_length = x.size(-1)\n",
        "    result_mask = pm & \\\n",
        "                  look_ahead_mask(seq_length).type_as(pm.data)\n",
        "    return result_mask"
      ],
      "metadata": {
        "id": "2yuMxRinE3uH"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FeedForward [1 Point]"
      ],
      "metadata": {
        "id": "Kh86csH_FCyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, hidden_size, ff_hidden_size, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        # TODO: MAKE FEED FORWARD\n",
        "        # It goes as linear -> RELU -> Dropout -> Linear\n",
        "        self.pre_linear = ...\n",
        "        self.post_linear = ...\n",
        "        self.dropout = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: WRITE CORRECT RETURN\n",
        "        return None\n",
        "\n",
        "def clone_layer(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "metadata": {
        "id": "4zy1PNwlGIEX"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_FF():\n",
        "    ff = FeedForward(24, 256)\n",
        "    assert(ff.pre_linear.weight.size() == (256, 24))\n",
        "    assert(ff.post_linear.weight.size() == (24, 256))\n",
        "\n",
        "    x = torch.rand(32, 24)\n",
        "    out = ff(x)\n",
        "    assert(out.size() == (32, 24))\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "0TeMntIjadpK"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_FF()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMA1EoGcatvk",
        "outputId": "68ed9acc-9e30-4fb8-f9bc-9b18f506481c"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test is passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwoQ_X8ylJYN"
      },
      "source": [
        "###  MultiHeadAttention [1.5 Point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYGVEp3mkgNf"
      },
      "source": [
        "\n",
        "Then you are required to implement `attention` method in the class  `MultiHeadAttention`. The MultiHeadAttention layer takes as input  query vectors, key and value vectors for each step of the sequence of matrices  Q,K,V correspondingly. Each key vector, value vector, and query vector is obtained as a result of linear projection using one of three trained vector parameter matrices from the previous layer. This semantics can be represented in the form of formulas:\n",
        "$$\n",
        "Attention(Q, K, V)=softmax\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V\\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "MultiHead(Q, K, V) = Concat\\left(head_1, ... , head_h\\right) W^O\\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "head_i=Attention\\left(Q W_i^Q, K W_i^K, V W_i^V\\right)\\\\\n",
        "$$\n",
        "$h$ - the number of attention heads - parallel sub-layers for Scaled Dot-Product Attention on a vector of smaller dimension ($d_{k} = d_{q} = d_{v} = d_{model} / h$).\n",
        "The logic of  \\texttt{MultiHeadAttention} is presented in the picture (from original  [paper](https://arxiv.org/abs/1706.03762)):\n",
        "\n",
        "![](https://lilianweng.github.io/lil-log/assets/images/transformer.png)\n",
        "\n",
        "\n",
        "Inside a method `attention` you are required to create a dropout layer from  MultiHeadAttention class constructor. Dropout layer is to be applied directly on the attention weights - the result of softmax operation. Value of drop probability  can be regulated in the train in the `model_config['dropout']['attention']`.\n",
        "\n",
        "The correctness of implementation can be checked with\n",
        "`test_multi_head_attention()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, hidden_size, dropout=None):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert hidden_size % n_heads == 0\n",
        "        self.head_hidden_size = hidden_size // n_heads\n",
        "        self.n_heads = n_heads\n",
        "        self.linears = clone_layer(nn.Linear(hidden_size, hidden_size), 4)\n",
        "        self.attn_weights = None\n",
        "        self.dropout = dropout\n",
        "        if self.dropout is not None:\n",
        "            self.dropout_layer = nn.Dropout(p=self.dropout)\n",
        "\n",
        "    def attention(self, query, key, value, mask):\n",
        "        \"\"\"Compute 'Scaled Dot Product Attention'\n",
        "            query, key and value tensors have the same shape:\n",
        "                (batch size, number of heads, sequence length, head hidden size)\n",
        "            mask shape: (batch size, 1, sequence length, sequence length)\n",
        "                '1' dimension value will be broadcasted to number of heads inside your operations\n",
        "            mask should be applied before using softmax to get attn_weights\n",
        "        \"\"\"\n",
        "        ## attn_weights shape: (batch size, number of heads, sequence length, sequence length)\n",
        "        ## output shape: (batch size, number of heads, sequence length, head hidden size)\n",
        "        ## TODO: provide your implementation here\n",
        "        ## don't forget to apply dropout to attn_weights if self.dropout is not None\n",
        "        raise NotImplementedError\n",
        "        return output, attn_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Split vectors for different attention heads (from hidden_size => n_heads x head_hidden_size)\n",
        "        # and do separate linear projection, for separate trainable weights\n",
        "        query, key, value = \\\n",
        "            [l(x).view(batch_size, -1, self.n_heads, self.head_hidden_size).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "        x, self.attn_weights = self.attention(query, key, value, mask=mask)\n",
        "        # x shape: (batch size, number of heads, sequence length, head hidden size)\n",
        "        # self.attn_weights shape: (batch size, number of heads, sequence length, sequence length)\n",
        "\n",
        "        # Concatenate the output of each head\n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "            .view(batch_size, -1, self.n_heads * self.head_hidden_size)\n",
        "\n",
        "        return self.linears[-1](x)"
      ],
      "metadata": {
        "id": "5q7mpdjnAVHP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_multi_head_attention():\n",
        "    mha = MultiHeadAttention(n_heads=1, hidden_size=5, dropout=None)\n",
        "    # batch_size == 2, sequence length == 3, hidden_size == 5\n",
        "    # query = torch.arange(150).reshape(2, 3, 5)\n",
        "    query = torch.tensor([[[[ 0.64144618, -0.95817388,  0.37432297,  0.58427106,\n",
        "          -0.94668716]],\n",
        "        [[-0.23199289,  0.66329209, -0.46507035, -0.54272512,\n",
        "          -0.98640698]],\n",
        "        [[ 0.07546638, -0.09277002,  0.20107185, -0.97407381,\n",
        "          -0.27713414]]],\n",
        "       [[[ 0.14727783,  0.4747886 ,  0.44992016, -0.2841419 ,\n",
        "          -0.81820319]],\n",
        "        [[-0.72324994,  0.80643179, -0.47655449,  0.45627872,\n",
        "           0.60942404]],\n",
        "        [[ 0.61712569, -0.62947282, -0.95215713, -0.38721959,\n",
        "          -0.73289725]]]])\n",
        "    key = torch.tensor([[[[-0.81759856, -0.60049991, -0.05923424,  0.51898901,\n",
        "          -0.3366209 ]],\n",
        "        [[ 0.83957818, -0.96361722,  0.62285191,  0.93452467,\n",
        "           0.51219613]],\n",
        "        [[-0.72758847,  0.41256154,  0.00490795,  0.59892503,\n",
        "          -0.07202049]]],\n",
        "       [[[ 0.72315339, -0.49896314,  0.94254637, -0.54356006,\n",
        "          -0.04837949]],\n",
        "        [[ 0.51759322, -0.43927061, -0.59924184,  0.92241702,\n",
        "          -0.86811696]],\n",
        "        [[-0.54322046, -0.92323003, -0.827746  ,  0.90842783,\n",
        "           0.88428119]]]])\n",
        "    value = torch.tensor([[[[-0.83895431,  0.805027  ,  0.22298283, -0.84849915,\n",
        "          -0.34906026]],\n",
        "        [[-0.02899652, -0.17456128, -0.17535998, -0.73160314,\n",
        "          -0.13468061]],\n",
        "        [[ 0.75234265,  0.02675947,  0.84766286, -0.5475651 ,\n",
        "          -0.83319316]]],\n",
        "       [[[-0.47834413,  0.34464645, -0.41921457,  0.33867964,\n",
        "           0.43470836]],\n",
        "        [[-0.99000979,  0.10220893, -0.4932273 ,  0.95938905,\n",
        "           0.01927012]],\n",
        "        [[ 0.91607137,  0.57395644, -0.90914179,  0.97212912,\n",
        "           0.33078759]]]])\n",
        "    query = query.float().transpose(1,2)\n",
        "    key = key.float().transpose(1,2)\n",
        "    value = value.float().transpose(1,2)\n",
        "\n",
        "    x,_ = torch.max(query[:,0,:,:], axis=-1)\n",
        "    mask = compositional_mask(x)\n",
        "    mask.unsqueeze_(1)\n",
        "    for n,t in [('query', query), ('key', key), ('value', value), ('mask', mask)]:\n",
        "        print(f'Name: {n}, shape: {t.size()}')\n",
        "    with torch.no_grad():\n",
        "        output, attn_weights = mha.attention(query, key, value, mask=mask)\n",
        "    assert output.size() == torch.Size([2,1,3,5])\n",
        "    assert attn_weights.size() == torch.Size([2,1,3,3])\n",
        "\n",
        "    truth_output = torch.tensor([[[[-0.8390,  0.8050,  0.2230, -0.8485, -0.3491],\n",
        "          [-0.6043,  0.5212,  0.1076, -0.8146, -0.2870],\n",
        "          [-0.0665,  0.2461,  0.3038, -0.7137, -0.4410]]],\n",
        "        [[[-0.4783,  0.3446, -0.4192,  0.3387,  0.4347],\n",
        "          [-0.7959,  0.1942, -0.4652,  0.7239,  0.1769],\n",
        "          [-0.3678,  0.2868, -0.5799,  0.7987,  0.2086]]]])\n",
        "    truth_attn_weights = torch.tensor([[[[1.0000, 0.0000, 0.0000],\n",
        "          [0.7103, 0.2897, 0.0000],\n",
        "          [0.3621, 0.3105, 0.3274]]],\n",
        "        [[[1.0000, 0.0000, 0.0000],\n",
        "          [0.3793, 0.6207, 0.0000],\n",
        "          [0.2642, 0.4803, 0.2555]]]])\n",
        "    # print(torch.abs(output - truth_output))\n",
        "    # print(torch.abs(attn_weights - truth_attn_weights))\n",
        "    assert torch.all(torch.abs(output - truth_output) < 1e-4).item()\n",
        "    assert torch.all(torch.abs(attn_weights - truth_attn_weights) < 1e-4).item()\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "ExHkza22FCF0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_multi_head_attention()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI6yMbX8FhGZ",
        "outputId": "45ff903e-d482-4158-92c6-2a2daf56b82d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: query, shape: torch.Size([2, 1, 3, 5])\n",
            "Name: key, shape: torch.Size([2, 1, 3, 5])\n",
            "Name: value, shape: torch.Size([2, 1, 3, 5])\n",
            "Name: mask, shape: torch.Size([2, 1, 3, 3])\n",
            "Test is passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "LUdLLSojGJbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "\n",
        "    def __init__(self, hidden_size, ff_hidden_size, n_heads, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                            dropout=dropout['attention'])\n",
        "        self.feed_forward = FeedForward(hidden_size, ff_hidden_size,\n",
        "                                        dropout=dropout['relu'])\n",
        "        self.sublayers = clone_layer(SublayerConnection(hidden_size, dropout['residual']), 2)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayers[1](x, self.feed_forward)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedder = Embedding(config['hidden_size'],\n",
        "                                  config['src_vocab_size'])\n",
        "        self.positional_encoder = PositionalEncoding(config['hidden_size'],\n",
        "                                                     max_len=config['max_src_seq_length'])\n",
        "        self.embedding_dropout = nn.Dropout(p=config['dropout']['embedding'])\n",
        "        self.encoder_layer = EncoderLayer(config['hidden_size'],\n",
        "                                          config['ff_hidden_size'],\n",
        "                                          config['n_heads'],\n",
        "                                          config['dropout'])\n",
        "        self.layers = clone_layer(self.encoder_layer, config['n_layers'])\n",
        "        self.layer_norm = LayerNorm(config['hidden_size'])\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        x = self.embedding_dropout(self.positional_encoder(self.embedder(x)))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "dFMMTX4NA0KP"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "6kyeSkMeGQo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder is made of 3 sublayers: self attention, encoder-decoder attention\n",
        "    and feed forward\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, ff_hidden_size, n_heads, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                            dropout=dropout['attention'])\n",
        "        self.encdec_attn = MultiHeadAttention(n_heads, hidden_size,\n",
        "                                              dropout=dropout['attention'])\n",
        "        self.feed_forward = FeedForward(hidden_size, ff_hidden_size,\n",
        "                                        dropout=dropout['relu'])\n",
        "        self.sublayers = clone_layer(SublayerConnection(hidden_size, dropout['residual']), 3)\n",
        "\n",
        "    def forward(self, x, encoder_output, encoder_mask, decoder_mask):\n",
        "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, decoder_mask))\n",
        "        x = self.sublayers[1](x, lambda x: self.encdec_attn(x, encoder_output,\n",
        "                                                            encoder_output, encoder_mask))\n",
        "        return self.sublayers[2](x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedder = Embedding(config['hidden_size'],\n",
        "                                  config['tgt_vocab_size'])\n",
        "        self.positional_encoder = PositionalEncoding(config['hidden_size'],\n",
        "                                                     max_len=config['max_tgt_seq_length'])\n",
        "        self.embedding_dropout = nn.Dropout(p=config['dropout']['embedding'])\n",
        "        self.decoder_layer = DecoderLayer(config['hidden_size'],\n",
        "                                          config['ff_hidden_size'],\n",
        "                                          config['n_heads'],\n",
        "                                          config['dropout'])\n",
        "        self.layers = clone_layer(self.decoder_layer, config['n_layers'])\n",
        "        self.layer_norm = LayerNorm(config['hidden_size'])\n",
        "\n",
        "    def forward(self, x, encoder_output, encoder_mask, decoder_mask):\n",
        "        x = self.embedding_dropout(self.positional_encoder(self.embedder(x)))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, encoder_mask, decoder_mask)\n",
        "        return self.layer_norm(x)"
      ],
      "metadata": {
        "id": "B4pSnS8NGPyf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "bwP_NVeYGY-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.config = config\n",
        "        self.encoder = Encoder(config)\n",
        "        self.decoder = Decoder(config)\n",
        "        self.proj = nn.Linear(config['hidden_size'], config['tgt_vocab_size'])\n",
        "\n",
        "        self.pad_idx = config['pad_idx']\n",
        "        self.tgt_vocab_size = config['tgt_vocab_size']\n",
        "\n",
        "    def encode(self, encoder_input, encoder_input_mask):\n",
        "        return self.encoder(encoder_input, encoder_input_mask)\n",
        "\n",
        "    def decode(self, encoder_output, encoder_input_mask, decoder_input, decoder_input_mask):\n",
        "        return self.decoder(decoder_input, encoder_output, encoder_input_mask, decoder_input_mask)\n",
        "\n",
        "    def linear_project(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        encoder_input_mask = padding_mask(encoder_input, pad_idx=self.config['pad_idx'])\n",
        "        decoder_input_mask = compositional_mask(decoder_input, pad_idx=self.config['pad_idx'])\n",
        "        encoder_output = self.encode(encoder_input, encoder_input_mask)\n",
        "        decoder_output = self.decode(encoder_output, encoder_input_mask,\n",
        "                                     decoder_input, decoder_input_mask)\n",
        "        output_logits = self.linear_project(decoder_output)\n",
        "        return output_logits\n",
        "\n",
        "\n",
        "def prepare_model(config):\n",
        "    model = Transformer(config)\n",
        "\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model"
      ],
      "metadata": {
        "id": "mVTRK_5cGYYa"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnmPBcVyrR6h"
      },
      "source": [
        "####  LrScheduler [1 Point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2luuBDZFrTj1"
      },
      "source": [
        "The last thing you have to prepare is the class  `LrScheduler`, which is in charge of  learning rate updating after every step of the optimizer. You are required to fill the class constructor and the method `learning_rate`. The preferable stratagy of updating the learning rate (lr), is the following two stages:\n",
        "\n",
        "* \"warmup\" stage - lr linearly increases until the defined value during the fixed number of steps (the proportion of all training steps - the parameter `train_config['warmup\\_steps\\_part']` in the train function).\n",
        "* \"decrease\" stage - lr linearly decreases until 0 during the left training steps.\n",
        "\n",
        "`learning_rate()` call should return the value of  lr at this step,  which number is stored at self.step. The class constructor takes not only `warmup_steps_part` but the peak learning rate value `lr_peak` at the end of \"warmup\" stage and a string name of the strategy of learning rate scheduling. You can test other strategies if you want to with `self.type attribute`.\n",
        "\n",
        "Correctness check: `test_lr_scheduler()`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LrScheduler:\n",
        "    def __init__(self, n_steps, **kwargs):\n",
        "        self.type = kwargs['type']\n",
        "        if self.type == 'warmup,decay_linear':\n",
        "            ## TODO: provide your implementation here\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            raise ValueError(f'Unknown type argument: {self.type}')\n",
        "        self._step = 0\n",
        "        self._lr = 0\n",
        "\n",
        "    def step(self, optimizer):\n",
        "        self._step += 1\n",
        "        lr = self.learning_rate()\n",
        "        for p in optimizer.param_groups:\n",
        "            p['lr'] = lr\n",
        "\n",
        "    def learning_rate(self, step=None):\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        if self.type == 'warmup,decay_linear':\n",
        "            ## TODO: provide your implementation here\n",
        "            raise NotImplementedError\n",
        "        return self._lr\n",
        "\n",
        "    def state_dict(self):\n",
        "        sd = copy.deepcopy(self.__dict__)\n",
        "        return sd\n",
        "\n",
        "    def load_state_dict(self, sd):\n",
        "        for k in sd.keys():\n",
        "            self.__setattr__(k, sd[k])\n"
      ],
      "metadata": {
        "id": "YDvKYF5EAdnX"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lr_scheduler():\n",
        "    lrs_type = 'warmup,decay_linear'\n",
        "    warmup_steps_part =  0.1\n",
        "    lr_peak = 3e-4\n",
        "    sch = LrScheduler(100, type=lrs_type, warmup_steps_part=warmup_steps_part,\n",
        "                      lr_peak=lr_peak)\n",
        "    assert sch.learning_rate(step=5) - 15e-5 < 1e-6\n",
        "    assert sch.learning_rate(step=10) - 3e-4 < 1e-6\n",
        "    assert sch.learning_rate(step=50) - 166e-6 < 1e-6\n",
        "    assert sch.learning_rate(step=100) - 0. < 1e-6\n",
        "    print('Test is passed!')"
      ],
      "metadata": {
        "id": "4JHOgJDBGjhr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lr_scheduler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Ys4DZRGmzM",
        "outputId": "5541a8ac-a76b-4dd4-da9d-b033fdbb21c9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test is passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run and translate [0.5 Points]"
      ],
      "metadata": {
        "id": "byCY6Tn-A9i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "\n",
        "def run_epoch(data_iter, model, lr_scheduler, optimizer, device, verbose=False):\n",
        "    start = time.time()\n",
        "    local_start = start\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    # TODO: TAKE CROSS ENTROPY LOSS WITH SUM REDUCTION\n",
        "    loss_fn =\n",
        "    for i, batch in tqdm(enumerate(data_iter)):\n",
        "        encoder_input = batch[0].to(device)\n",
        "        decoder_input = batch[1].to(device)\n",
        "        decoder_target = batch[2].to(device)\n",
        "        # TODO: OBTAIN MODEL LOGITS, PASS THEM TO LOSS_FN AND CALCULATE LOSS\n",
        "        loss = ...\n",
        "        total_loss += loss.item()\n",
        "        batch_n_tokens = (decoder_target != model.pad_idx).sum().item()\n",
        "        total_tokens += batch_n_tokens\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad()\n",
        "            lr_scheduler.step(optimizer)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        tokens += batch_n_tokens\n",
        "        if verbose and i % 1000 == 1:\n",
        "            elapsed = time.time() - local_start\n",
        "            print(\"batch number: %d, accumulated average loss: %f, tokens per second: %f\" %\n",
        "                  (i, total_loss / total_tokens, tokens / elapsed))\n",
        "            local_start = time.time()\n",
        "            tokens = 0\n",
        "\n",
        "    average_loss = total_loss / total_tokens\n",
        "    print('** End of epoch, accumulated average loss = %f **' % average_loss)\n",
        "    epoch_elapsed_time = format_time(time.time() - start)\n",
        "    print(f'** Elapsed time: {epoch_elapsed_time}**')\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, model, lr_scheduler, optimizer, model_dir_path):\n",
        "    save_path = os.path.join(model_dir_path, f'cpkt_{epoch}_epoch')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'lr_scheduler_state_dict': lr_scheduler.state_dict()\n",
        "    }, save_path)\n",
        "    print(f'Saved checkpoint to {save_path}')\n",
        "\n",
        "def load_model(epoch, model_dir_path):\n",
        "    save_path = os.path.join(model_dir_path, f'cpkt_{epoch}_epoch')\n",
        "    checkpoint = torch.load(save_path)\n",
        "    with open(os.path.join(model_dir_path, 'model_config.json'), 'r', encoding='utf-8') as rf:\n",
        "        model_config = json.load(rf)\n",
        "    model = prepare_model(model_config)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model\n",
        "\n",
        "def greedy_decode(model, device, encoder_input, max_len, start_symbol):\n",
        "    batch_size = encoder_input.size()[0]\n",
        "    decoder_input = torch.ones(batch_size, 1).fill_(start_symbol).type_as(encoder_input.data).to(device)\n",
        "\n",
        "    for i in range(max_len):\n",
        "        logits = model(encoder_input, decoder_input)\n",
        "\n",
        "        _, predicted_ids = torch.max(logits, dim=-1)\n",
        "        next_word = predicted_ids[:, i]\n",
        "        # print(next_word)\n",
        "        rest = torch.ones(batch_size, 1).type_as(decoder_input.data)\n",
        "        # print(rest[:,0].size(), next_word.size())\n",
        "        rest[:, 0] = next_word\n",
        "        decoder_input = torch.cat([decoder_input, rest], dim=1).to(device)\n",
        "        # print(decoder_input)\n",
        "    return decoder_input\n",
        "\n",
        "def generate_predictions(dataloader, max_decoding_len, text_encoder, model, device):\n",
        "    # print(f'Max decoding length = {max_decoding_len}')\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    start_token_id = text_encoder.service_vocabs['token2id'][\n",
        "        text_encoder.service_token_names['start_token']]\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            encoder_input = batch[0].to(device)\n",
        "            prediction_tensor = \\\n",
        "                greedy_decode(model, device, encoder_input, max_decoding_len,\n",
        "                              start_token_id)\n",
        "\n",
        "            predictions.extend([''.join(e) for e in text_encoder.id2token(prediction_tensor.cpu().numpy(),\n",
        "                                                                          unframe=True, lang_key='ru')])\n",
        "    return np.array(predictions)\n",
        "\n",
        "\n",
        "def train(source_strings, target_strings):\n",
        "    '''Common training cycle for final run (fixed hyperparameters,\n",
        "    no evaluation during training)'''\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f'Using GPU device: {device}')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(f'GPU is not available, using CPU device {device}')\n",
        "\n",
        "    train_df = pd.DataFrame({'en': source_strings, 'ru': target_strings})\n",
        "    text_encoder = TextEncoder()\n",
        "    text_encoder.make_vocabs(train_df)\n",
        "\n",
        "    # TODO: MOVE CONFIG TO ARGUMENTS\n",
        "    model_config = {\n",
        "        'src_vocab_size': text_encoder.src_vocab_size,\n",
        "        'tgt_vocab_size': text_encoder.tgt_vocab_size,\n",
        "        'max_src_seq_length': max(train_df['en'].aggregate(len)) + 2, #including start_token and end_token\n",
        "        'max_tgt_seq_length': max(train_df['ru'].aggregate(len)) + 2,\n",
        "        'n_layers': 2,\n",
        "        'n_heads': 2,\n",
        "        'hidden_size': 128,\n",
        "        'ff_hidden_size': 256,\n",
        "        'dropout': {\n",
        "            'embedding': 0.1,\n",
        "            'attention': 0.1,\n",
        "            'residual': 0.1,\n",
        "            'relu': 0.1\n",
        "        },\n",
        "        'pad_idx': 0\n",
        "    }\n",
        "    model = prepare_model(model_config)\n",
        "    model.to(device)\n",
        "\n",
        "    # TODO: MOVE CONFIG TO ARGUMENTS\n",
        "    train_config = {'batch_size': 200, 'n_epochs': 1, 'lr_scheduler': {\n",
        "        'type': 'warmup,decay_linear',\n",
        "        'warmup_steps_part': 0.1,\n",
        "        'lr_peak': 3e-4,\n",
        "    }}\n",
        "\n",
        "    #Model training procedure\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.)\n",
        "    n_steps = (len(train_df) // train_config['batch_size'] + 1) * train_config['n_epochs']\n",
        "    lr_scheduler = LrScheduler(n_steps, **train_config['lr_scheduler'])\n",
        "\n",
        "    # prepare train data\n",
        "    source_strings, target_strings = zip(*sorted(zip(source_strings, target_strings),\n",
        "                                                 key=lambda e: len(e[0])))\n",
        "    train_dataloader = create_dataloader(source_strings, target_strings, text_encoder,\n",
        "                                         train_config['batch_size'],\n",
        "                                         shuffle_batches_each_epoch=True)\n",
        "    # training cycle\n",
        "    for epoch in range(1,train_config['n_epochs']+1):\n",
        "        print('\\n' + '-'*40)\n",
        "        print(f'Epoch: {epoch}')\n",
        "        print(f'Run training...')\n",
        "        model.train()\n",
        "        run_epoch(train_dataloader, model,\n",
        "                  lr_scheduler, optimizer, device=device, verbose=False)\n",
        "    learnable_params = {\n",
        "        'model': model,\n",
        "        'text_encoder': text_encoder,\n",
        "    }\n",
        "    return learnable_params\n",
        "\n",
        "def classify(source_strings, learnable_params):\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f'Using GPU device: {device}')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(f'GPU is not available, using CPU device {device}')\n",
        "\n",
        "    model = learnable_params['model']\n",
        "    text_encoder = learnable_params['text_encoder']\n",
        "    batch_size = 200\n",
        "    dataloader = create_dataloader(source_strings, None, text_encoder,\n",
        "                                   batch_size, shuffle_batches_each_epoch=False)\n",
        "    max_decoding_len = model.config['max_tgt_seq_length']\n",
        "    predictions = generate_predictions(dataloader, max_decoding_len, text_encoder, model, device)\n",
        "    #return single top1 prediction for each sample\n",
        "    return np.expand_dims(predictions, 1)"
      ],
      "metadata": {
        "id": "-K7-KJEGA8po"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "jpG6i8X-HMmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PREDS_FNAME = \"preds_translit.tsv\"\n",
        "SCORED_PARTS = ('train', 'dev', 'train_small', 'dev_small', 'test')\n",
        "TRANSLIT_PATH = \"TRANSLIT\""
      ],
      "metadata": {
        "id": "f-7-YtzEKnug"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 1\n",
        "part2ixy = load_dataset(TRANSLIT_PATH, parts=SCORED_PARTS)\n",
        "\n",
        "train_ids, train_strings, train_transliterations = part2ixy['train']\n",
        "print('\\nTraining classifier on %d examples from train set ...' % len(train_strings))\n",
        "st = time.time()\n",
        "params = train(train_strings, train_transliterations)\n",
        "print('Classifier trained in %.2fs' % (time.time() - st))"
      ],
      "metadata": {
        "id": "74GcLUTuLFyS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "475d4a3b-921b-4f36-8a63-ff81812b3ad8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training classifier on 105371 examples from train set ...\n",
            "Using GPU device: cuda\n",
            "\n",
            "----------------------------------------\n",
            "Epoch: 1\n",
            "Run training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "527it [00:16, 31.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** End of epoch, accumulated average loss = 3.165620 **\n",
            "** Elapsed time: 0:00:17**\n",
            "Classifier trained in 17.47s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allpreds = []\n",
        "for part, (ids, x, y) in part2ixy.items():\n",
        "    print('\\nClassifying %s set with %d examples ...' % (part, len(x)))\n",
        "    st = time.time()\n",
        "    preds = classify(x, params)\n",
        "    print('%s set classified in %.2fs' % (part, time.time() - st))\n",
        "    count_of_values = list(map(len, preds))\n",
        "    assert np.all(np.array(count_of_values) == top_k)\n",
        "    #score(preds, y)\n",
        "    allpreds.extend(zip(ids, preds))\n",
        "\n",
        "save_preds(allpreds, preds_fname=PREDS_FNAME)\n",
        "print('\\nChecking saved predictions ...')\n",
        "score_preds(preds_path=PREDS_FNAME, data_dir=TRANSLIT_PATH, parts=SCORED_PARTS)"
      ],
      "metadata": {
        "id": "hPELZcXeHLHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4714f9-0eea-43a4-bab3-e4750655eb05"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classifying train set with 105371 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 527/527 [01:23<00:00,  6.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set classified in 83.06s\n",
            "\n",
            "Classifying dev set with 26342 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 132/132 [00:19<00:00,  6.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev set classified in 19.90s\n",
            "\n",
            "Classifying train_small set with 2000 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:01<00:00,  6.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_small set classified in 1.45s\n",
            "\n",
            "Classifying dev_small set with 2000 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:01<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev_small set classified in 1.94s\n",
            "\n",
            "Classifying test set with 32926 examples ...\n",
            "Using GPU device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 165/165 [00:25<00:00,  6.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set classified in 25.50s\n",
            "Predictions saved to preds_translit.tsv\n",
            "\n",
            "Checking saved predictions ...\n",
            "train set accuracy@1: 0.00\n",
            "dev set accuracy@1: 0.00\n",
            "train_small set accuracy@1: 0.00\n",
            "dev_small set accuracy@1: 0.00\n",
            "no labels for test set\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'acc@1': 9.490277210997334e-06},\n",
              " 'dev': {'acc@1': 0.0},\n",
              " 'train_small': {'acc@1': 0.0},\n",
              " 'dev_small': {'acc@1': 0.0}}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFfDH0-SsRm-"
      },
      "source": [
        "###  Hyper-parameters choice [5 Points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxqZbEmtsV0g"
      },
      "source": [
        "The model is ready. Now we need to find the optimal hyper-parameters.\n",
        "\n",
        "The quality of models with different hyperparameters should be monitored on dev or on dev_small samples (in order to save time, since generating transliterations is a rather time-consuming process, comparable to one training epoch).\n",
        "\n",
        "To generate predictions, you can use the `generate_predictions` function, to calculate the accuracy@1 metric, and then you can use the `compute_metrics` function.\n",
        "\n",
        "\n",
        "\n",
        "Hyper-parameters are stored in the dictionary `model_config` and `train_config` in train function. The following hyperparameters in `model_config` and `train_config` are suggested to leave unmodified:\n",
        "\n",
        "* n_layers $=$ 2\n",
        "* n_heads $=$ 2\n",
        "* hidden_size $=$ 128\n",
        "* fc_hidden_size $=$ 256\n",
        "* warmup_steps_part $=$ 0.1\n",
        "* batch_size $=$ 200\n",
        "\n",
        " You can vary the dropout value. The model has 4 types of : ***embedding dropout*** applied on embdeddings before sending to the first layer of  Encoder or Decoder, ***attention*** dropout applied on the attention weights in the MultiHeadAttention layer, ***residual dropout*** applied on the output of each sublayer (MultiHeadAttention or FeedForward) in layers Encoder and Decoder and, finaly, ***relu dropout*** in used in FeedForward layer. For all 4 types it is suggested to test the same value of dropout from the list: 0.1, 0.15, 0.2.\n",
        " Also it is suggested to test several peak levels of learning rate - **lr_peak** : 5e-4, 1e-3, 2e-3.\n",
        "\n",
        "Note that if you are using a GPU, then training one epoch takes about 1 minute, and up to 1 GB of video memory is required. When using the CPU, the learning speed slows down by about 2 times. If there are problems with insufficient RAM / video memory, reduce the batch size, but in this case the optimal range of learning rate values will change, and it must be determined again. To train a model with  batch_size $=$ 200 , it will take at least 300 epochs to achieve accuracy 0.66 on dev_small dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQXVmzk0a60Y"
      },
      "source": [
        "*Question: What are the optimal hyperpameters according to your experiments? Add plots or other descriptions here.*\n",
        "\n",
        "```\n",
        "\n",
        "ENTER HERE YOUR ANSWER\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER HERE YOUR CODE"
      ],
      "metadata": {
        "id": "ylE7wnrXJqmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hMYmIO2tf8z"
      },
      "source": [
        "## Label smoothing [1 Point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setYzbjCtqZY"
      },
      "source": [
        "Now imagine that we have a prediction vector from probabilities at position t in the sequence of tokens for each token id from the vocabulary. CrossEntropy compares it with ground truth one-hot representation\n",
        "\n",
        "$$[0, ... 0, 1, 0, ..., 0].$$\n",
        "\n",
        "And now imagine that we are slightly \"smoothed\" the values in the ground truth vector and obtained\n",
        "\n",
        "$$[\\frac{\\alpha}{|V|}, ..., \\frac{\\alpha}{|V|}, 1(1-\\alpha)+\\frac{\\alpha}{|V|},  \\frac{\\alpha}{|V|}, ... \\frac{\\alpha}{|V|}],$$\n",
        "\n",
        "where $\\alpha$ - parameter from 0 to 1, $|V|$ - vocabulary size - number of components in the ground truth vector. The values ​​of this new vector are still summed to 1. Calculate the cross-entropy of our prediction vector and the new ground truth. Now, firstly, cross-entropy will never reach 0, and secondly, the result of the error function will require the model, as usual, to return the highest probability vector compared to other components of the probability vector for the correct token in the dictionary, but at the same time not too large, because as the value of this probability approaches 1, the value of the error function increases. For research on the use of label smoothing, see the [paper](https://arxiv.org/abs/1906.02629).\n",
        "    \n",
        "Accordingly, in order to embed label smoothing into the model, it is necessary to carry out the transformation described above on the ground truth vectors, as well as to implement the cross-entropy calculation, since the used `torch.nn.CrossEntropy` class is not quite suitable, since for the ground truth representation of `__call__` method takes the id of the correct token and builds a one-hot vector already inside. However, it is possible to implement what is required based on the internal implementation of this class [CrossEntropyLoss](https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#CrossEntropyLoss).\n",
        "    \n",
        "\n",
        "Test different values of $\\alpha$ (e.x, 0.05, 0.1, 0.2). Describe your experiments and results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL9V_9-7bVzw"
      },
      "source": [
        "```\n",
        "\n",
        "ENTER HERE YOUR ANSWER\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTER HERE YOUR CODE"
      ],
      "metadata": {
        "id": "9ZU-88SKPj36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS: Additional Experiments [Up to 2 Points]\n",
        "\n",
        "Be creative and run additional experiments with changing Transformer Architecture to get better score. You probably would like to dive into\n",
        "\n",
        "1) Reading papers\n",
        "\n",
        "2) Play with Normalization\n",
        "\n",
        "3) Play with Positional Encoding\n",
        "\n",
        "4) Try different Schedulers\n",
        "\n",
        "5) Maybe you would like to alter architecture a lot :)\n",
        "\n",
        "The fair and interesting experiments will be highly rewarded even if the result was not successful. Nevertheless, you need to report them properly"
      ],
      "metadata": {
        "id": "sTzR27d4desF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jg9hJ6hgeLqf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
